{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e05d0d-01eb-4ad3-8e34-186f85a6fcfd",
   "metadata": {},
   "source": [
    "## Conservation Genomics Practical\n",
    "\n",
    "------------------------ \n",
    "**Background**:\n",
    "\n",
    "The primary data for modern conservation genomics efforts often includes whole-genome\n",
    "sequences of multiple individuals of a threatened species. From this data, we infer the levels\n",
    "and patterns of genetic diversity, and apply population genetics models to infer past\n",
    "demography. We can also infer levels of inbreeding and patterns of migration from sequence\n",
    "data. If we have access to information about the deleterious consequences of variants, or we\n",
    "apply computation inference of those deleterious effects, we can further infer\n",
    "<br>\n",
    "<br>\n",
    "I have four sets of sequence data, all drawn from one chromosome, from four different populations with distinct population histories. The data consist of only the segregating sites, with “0” being the ancestral allele, and “1” being the derived allele. Each row represents a haplotype drawn from the population, and each dataset consists of 50 such\n",
    "haplotypes.\n",
    "I will randomly select one of the blocks from each data file, and, for each data set, I will:\n",
    "1. Read in the data into a matrix\n",
    "2. Calculate the nucleotide diversity, pi (the data are the segregating sites **only**, out of a span of 10 kb).\n",
    "3. Determine the number of segregating sites, S.\n",
    "4. Calculate Watterson’s theta.\n",
    "5. Calculate Tajima’s D\n",
    "\n",
    "**Equations:** <br>\n",
    "Watterson's estimator ($\\theta$) <br>\n",
    "&nbsp; $\\theta=\\frac{S}{\\sum \\limits_{i=1}^{n-1}1/i}$ &nbsp; ; &nbsp; where $S$=segregating sites, $n$=the number of loci (columns). <br>\n",
    "<br>\n",
    "Average nucleotide diversity ($\\pi$) <br>\n",
    "&nbsp; $ \\pi=\\frac{1}{L} \\sum\\limits_{k=1}^{L}h_k$ &nbsp; ; &nbsp; where $h_{k}=\\frac{2p(n-p)}{n(n-1)}$ , $L$=number of loci (i.e. nucelotide positions / columns), $n$=sample size (number of haplotypes/rows) , and $p$=number of derived alleles (1's) at loci $k$. <br>\n",
    "<br>\n",
    "Tajima's $D$ <br>\n",
    "$D=\\frac{\\pi-\\theta}{SD(\\pi-\\theta)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7579086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import random\n",
    "#from scipy.stats import tajima_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4adac9e2-eade-45b9-9e0a-eb01b4642e47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4   5   6   7   8   9   ...  62  63  64  65  66  67  68  \\\n",
      "0    0   0   1   0   0   0   0   1   0   0  ...   0   0   1   0   1   0   1   \n",
      "1    1   0   1   0   0   0   0   1   0   0  ...   0   1   0   0   1   0   1   \n",
      "2    0   0   1   0   0   1   0   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "3    0   0   1   0   0   1   0   1   1   0  ...   0   0   0   0   1   0   1   \n",
      "4    0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "5    0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "6    0   0   1   0   1   0   0   1   0   1  ...   0   0   1   0   1   0   1   \n",
      "7    0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "8    0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "9    0   0   1   0   0   1   0   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "10   0   1   0   0   0   0   0   0   0   0  ...   1   0   0   0   0   0   0   \n",
      "11   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "12   0   0   1   0   1   0   0   1   0   1  ...   0   0   1   0   1   0   1   \n",
      "13   0   0   1   0   0   0   0   1   0   0  ...   0   0   1   0   1   0   1   \n",
      "14   0   0   1   0   0   0   0   1   0   0  ...   0   0   1   0   1   0   1   \n",
      "15   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "16   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "17   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "18   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   1   0   \n",
      "19   0   0   1   0   0   0   0   1   0   0  ...   0   0   1   0   1   0   1   \n",
      "20   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "21   0   0   1   0   0   0   0   1   0   0  ...   0   0   1   0   1   0   1   \n",
      "22   0   1   0   0   0   0   0   0   0   0  ...   1   0   0   0   0   0   0   \n",
      "23   0   0   1   0   1   0   0   1   0   1  ...   0   0   1   1   1   0   1   \n",
      "24   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "25   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "26   1   0   1   0   0   0   0   1   0   0  ...   0   1   0   0   1   0   1   \n",
      "27   0   0   1   0   1   0   0   1   0   1  ...   0   0   1   1   1   0   1   \n",
      "28   0   0   1   0   0   1   0   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "29   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "30   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "31   0   0   1   0   1   0   0   1   0   1  ...   0   0   1   1   1   0   1   \n",
      "32   0   0   1   0   1   0   0   1   0   1  ...   0   0   1   1   1   0   1   \n",
      "33   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "34   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "35   1   0   1   0   0   0   0   1   0   0  ...   0   1   0   0   1   0   1   \n",
      "36   0   0   1   0   0   1   1   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "37   0   0   1   0   0   1   0   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "38   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "39   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "40   0   1   0   0   0   0   0   0   0   0  ...   1   0   0   0   0   0   0   \n",
      "41   0   0   1   0   0   1   0   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "42   0   0   1   0   0   1   0   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "43   0   0   1   0   0   1   0   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "44   0   0   1   0   1   0   0   1   0   1  ...   0   0   1   0   1   0   1   \n",
      "45   0   0   1   0   0   1   0   1   0   0  ...   0   0   0   0   1   0   1   \n",
      "46   0   0   1   1   0   0   0   1   0   0  ...   0   0   1   0   1   0   1   \n",
      "47   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "48   0   0   1   0   1   0   0   1   0   1  ...   0   0   1   0   1   0   1   \n",
      "49   0   0   1   0   0   1   0   1   1   0  ...   0   0   0   0   1   0   1   \n",
      "\n",
      "    69  70  71  \n",
      "0    1   0   0  \n",
      "1    0   0   1  \n",
      "2    0   0   0  \n",
      "3    0   0   0  \n",
      "4    0   1   0  \n",
      "5    0   1   0  \n",
      "6    0   0   0  \n",
      "7    0   1   0  \n",
      "8    0   1   0  \n",
      "9    0   0   0  \n",
      "10   0   1   0  \n",
      "11   0   1   0  \n",
      "12   0   0   0  \n",
      "13   0   0   0  \n",
      "14   1   0   0  \n",
      "15   0   1   0  \n",
      "16   0   1   0  \n",
      "17   0   1   0  \n",
      "18   0   1   0  \n",
      "19   0   0   0  \n",
      "20   0   1   0  \n",
      "21   0   0   0  \n",
      "22   0   1   0  \n",
      "23   0   0   0  \n",
      "24   0   1   0  \n",
      "25   0   1   0  \n",
      "26   0   0   1  \n",
      "27   0   0   0  \n",
      "28   0   0   0  \n",
      "29   0   1   0  \n",
      "30   0   1   0  \n",
      "31   0   0   0  \n",
      "32   0   0   0  \n",
      "33   0   1   0  \n",
      "34   0   1   0  \n",
      "35   0   0   1  \n",
      "36   0   0   0  \n",
      "37   0   0   0  \n",
      "38   0   1   0  \n",
      "39   0   1   0  \n",
      "40   0   1   0  \n",
      "41   0   0   0  \n",
      "42   0   0   0  \n",
      "43   0   0   0  \n",
      "44   0   0   0  \n",
      "45   0   0   0  \n",
      "46   0   0   0  \n",
      "47   0   1   0  \n",
      "48   0   0   0  \n",
      "49   0   0   0  \n",
      "\n",
      "[50 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "# Input matrices \n",
    "def load_haplotype_data(file_path):\n",
    "    with open(file_path, 'r') as file:    # 'r' = read mode\n",
    "        blocks = file.read().split('\\n\\n')   #split file in to blocks from newlines\n",
    "        block = random.choice(blocks)\n",
    "        processed_lines = []\n",
    "        # from each file, select one block at random,\n",
    "        for line in block.strip().split('\\n'):\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line.replace(\",\", \"\").isdigit():\n",
    "                processed_lines.append(stripped_line.split(','))\n",
    "        return pd.DataFrame(processed_lines).astype(int) \n",
    "    # each block consists of haplotypes from 50 individuals (rows)\n",
    "\n",
    "# Initialize a list to store each haplotype matrix\n",
    "haplotype_blocks = []\n",
    "\n",
    "# Need to do the following for all 4 blocks selected from the 4 files.\n",
    "#hapmx = pd.read_csv('ConGensData1block1.txt', sep = ',', header=None)\n",
    "hapmx1 = load_haplotype_data('Data/ConsGenData1.txt')\n",
    "hapmx2 = load_haplotype_data('Data/ConsGenData2.txt')\n",
    "hapmx3 = load_haplotype_data('Data/ConsGenData3.txt')\n",
    "hapmx4 = load_haplotype_data('Data/ConsGenData4.txt')\n",
    "# hapmx1 = pd.read_csv('Data/ConsGenData1.txt', sep=',', header=None)\n",
    "# hapmx2 = pd.read_csv('Data/ConsGenData2.txt', sep=',', header=None)\n",
    "# hapmx3 = pd.read_csv('Data/ConsGenData3.txt', sep=',', header=None)\n",
    "# hapmx4 = pd.read_csv('Data/ConsGenData4.txt', sep=',', header=None)\n",
    "\n",
    "haplotype_blocks.extend([hapmx1, hapmx2, hapmx3, hapmx4])\n",
    "\n",
    "print(haplotype_blocks[0]) #display first matrix for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58051c04-27b4-4490-be14-0a4d7a89692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate nucelotide diversity (pi)\n",
    "def calc_pi(haplotypes):\n",
    "    # variables\n",
    "    # formula\n",
    "    # calculations\n",
    "    return pi\n",
    "\n",
    "result_pi = calc_pi(hapmx)\n",
    "print(f\"Nucelotide diversity (pi): {result_pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b58d4-7b8c-45ff-8be4-cefded451f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the number of segregating sites (S)\n",
    "def count_segregating_sites(haplotypes):\n",
    "    n = len(haplotypes.axes[1])\n",
    "    S = 0\n",
    "    for i in range(0,n):\n",
    "        if len(haplotypes[i].unique()) == 1: \n",
    "            S = S\n",
    "        else:\n",
    "            S = S + 1\n",
    "    return S\n",
    "\n",
    "result_S = count_segregating_sites(hapmx)\n",
    "print(f\"Segregating sites: {result_S} out of\" {n} \"sites.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4835e04-d93c-4260-bb67-ca0e3d596c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wattersons theta\n",
    "def calc_watterson(haplotypes):\n",
    "    # S = segregating sites\n",
    "    # a = sum(1/i)\n",
    "    # pi = S / a ...\n",
    "    return watt\n",
    "\n",
    "result_watt = calculate_watterson(hapmx)\n",
    "print(f\"Wattersons theta: {result_watt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20781ebc-e7ed-48bf-b092-63106fce8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Tajima's D\n",
    "def tajimas_d(haplotypes):\n",
    "    # variable\n",
    "    # formula\n",
    "    # calculations\n",
    "    Tajimas_D = numerator / denominator\n",
    "    return Tajimas_D\n",
    "\n",
    "result_D = tajimas_d(haplotype_matrix)\n",
    "print(f\"Tajima's D: {result_D}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
